redis:
  fullnameOverride: redis
  image:
    registry: docker.io
    repository: bitnamilegacy/redis
  global:
    storageClass: longhorn
  auth:
    enabled: true
    existingSecret: redis-creds
    existingSecretPasswordKey: REDIS_PASSWORD
    sentinel: false
  # Performance tuning for Harbor Redis
  commonConfiguration: |-
    maxmemory 4gb
    maxmemory-policy volatile-lru
    appendonly yes
    save 900 1
    save 300 10
    save 60 10000
    tcp-keepalive 300
    activerehashing yes
    lazyfree-lazy-eviction yes
    lazyfree-lazy-expire yes
    io-threads 4
    io-threads-do-reads yes

  architecture: replication
  replica:
    replicaCount: 3
    disableCommands:
    - FLUSHDB
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2
        memory: 6Gi
    persistence:
      enabled: true
      storageClass: longhorn
      size: 60Gi
    terminationGracePeriodSeconds: 60
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 30
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 15
      timeoutSeconds: 15
      successThreshold: 1
      failureThreshold: 10

  kubectl:
    image:
      registry: docker.io
      repository: bitnamilegacy/kubectl

  sentinel:
    enabled: true
    image:
      registry: docker.io
      repository: bitnamilegacy/redis-sentinel
    masterSet: mymaster
    quorum: 2
    replicas: 3
    terminationGracePeriodSeconds: 60
    persistence:
      enabled: true
      storageClass: longhorn
      size: 60Gi
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 15
      failureThreshold: 20
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 10

  # Disable volumePermissions - not supported on NFS storage
  # Instead, use podSecurityContext to run as correct user
  volumePermissions:
    enabled: false
  # Run Redis as user 1001 (bitnami default) to avoid permission issues on NFS
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  containerSecurityContext:
    enabled: true
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true

  metrics:
    enabled: true
    image:
      registry: docker.io
      repository: bitnamilegacy/redis-exporter
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 5
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 5
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 3

harbor:
  existingSecretAdminPassword: harbor
  existingSecretAdminPasswordKey: HARBOR_ADMIN_PASSWORD
  existingSecretSecretKey: harbor

  # Stakater Reloader - automatically reload pods when secrets change
  core:
    # Using single replica to avoid blob upload offset errors
    # Harbor core proxies /v2/ API calls to the registry, and with multiple
    # core pods, chunked uploads can hit different cores causing offset mismatches
    replicas: 1
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 4
        memory: 4Gi
    # Probe configuration - default 1s timeout causes restarts under load
    startupProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 30
    livenessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
    readinessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 5
    extraEnvVars:
    - name: OIDC_CLIENT_ID
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc-client-id
    - name: OIDC_CLIENT_SECRET
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc-client-secret
    - name: OIDC_ENDPOINT
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc_issuer-url
    - name: CONFIG_OVERWRITE_JSON
      value: |
        {
          "auth_mode": "oidc_auth",
          "oidc_name": "OIDC",
          "oidc_endpoint": "$(OIDC_ENDPOINT)",
          "oidc_groups_claim": "groups",
          "oidc_admin_group": "superUsers",
          "oidc_client_id": "$(OIDC_CLIENT_ID)",
          "oidc_client_secret": "$(OIDC_CLIENT_SECRET)",
          "oidc_scope": "openid,email,groups,name",
          "oidc_verify_cert": "true",
          "oidc_auto_onboard": "true",
          "oidc_user_claim": "name"
        }

  jobservice:
    replicas: 3
    secret: ""
    podAnnotations:
      reloader.stakater.com/auto: "true"
    jobLoggers:
    - database
    # Increased workers for parallel job processing
    maxJobWorkers: 20
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 2Gi
    # Init container to wait for core to be fully ready before starting jobservice
    # This prevents CrashLoopBackOff during rolling updates by ensuring:
    # 1. Core ping endpoint responds
    # 2. Core health endpoint reports healthy status
    # 3. Additional delay to ensure internal APIs are initialized
    initContainers:
    - name: wait-for-core
      image: busybox:1.36
      command:
      - sh
      - -c
      - |
        echo "Waiting for harbor-core to be ready..."
        # Wait for ping endpoint
        until wget -q --spider http://harbor-core:80/api/v2.0/ping 2>/dev/null; do
          echo "harbor-core ping not ready, waiting 5s..."
          sleep 5
        done
        echo "harbor-core ping is responding"
        # Wait for health endpoint to report healthy
        until wget -qO- http://harbor-core:80/api/v2.0/health 2>/dev/null | grep -q '"status":"healthy"'; do
          echo "harbor-core health not ready, waiting 5s..."
          sleep 5
        done
        echo "harbor-core health check passed"
        # Additional delay to ensure internal config API is fully initialized
        echo "Waiting 10s for internal APIs to initialize..."
        sleep 10
        echo "harbor-core is fully ready!"
    # Probe configuration - default 1s timeout causes restarts under load
    startupProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 30
    livenessProbe:
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
    readinessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 5

  registry:
    # Using single replica to avoid "upload resumed at wrong offset" errors
    # The Docker Registry stores blob upload state locally per pod, so with
    # multiple replicas and chunked uploads, requests can hit different pods
    # causing upload state mismatch. This is a known limitation of the
    # Docker Registry with multi-replica deployments.
    # TODO: Consider using shared storage or external blob upload state when available
    replicas: 1
    secret: ""
    podAnnotations:
      reloader.stakater.com/auto: "true"
    credentials:
      # username is defined in existingSecret below, but this value also used by jobservice, so let's align it
      username: harbor_registry_user
      # we use harbor secret to consolidate all the secrets
      # Secret keys must be REGISTRY_PASSWD and REGISTRY_HTPASSWD. For key REGISTRY_PASSWD the value is the password.
      # For key REGISTRY_HTPASSWD the value is the string in the password file generated by htpasswd where the username
      # is harbor_registry_user and the encryption type is bcrypt.
      # For example: `htpasswd -bBc passwordfile harbor_registry_user harbor_registry_password`. The username must be harbor_registry_user!
      existingSecret: harbor
    registry:
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: 4
          memory: 8Gi
    controller:
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 2Gi
    # trivy:
    #   # enabled the flag to enable Trivy scanner
    #   enabled: true
    gitHubToken: ""
  fullnameOverride: harbor
  externalURL: https://harbor.ops.techsecom.io
  ipFamily:
    # ipv6Enabled set to true if ipv6 is enabled in cluster, currently it affected the nginx related component
    ipv6:
      enabled: false
  expose:
    tls:
      enabled: true
      certSource: secret
      secret:
        secretName: harbor-tls
    ingress:
      className: nginx
      annotations:
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: "Harbor"
        gethomepage.dev/description: "Container Registry"
        gethomepage.dev/group: "Platform"
        gethomepage.dev/icon: "harbor.png"
        gethomepage.dev/href: "https://harbor.ops.techsecom.io"
        gethomepage.dev/pod-selector: "app=harbor"
        ingress.kubernetes.io/ssl-redirect: "true"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        # Extended timeouts for large image uploads (e.g., 27GB Windows ISOs)
        nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "1800"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "1800"
        nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
        # Session affinity to ensure same client uses same registry pod during uploads
        # This prevents "upload resumed at wrong offset" errors with multi-replica registry
        nginx.ingress.kubernetes.io/affinity: "cookie"
        nginx.ingress.kubernetes.io/affinity-mode: "persistent"
        nginx.ingress.kubernetes.io/session-cookie-name: "harbor-sticky"
        nginx.ingress.kubernetes.io/session-cookie-expires: "172800"
        nginx.ingress.kubernetes.io/session-cookie-max-age: "172800"
        nginx.ingress.kubernetes.io/session-cookie-change-on-failure: "true"
        # external-dns.alpha.kubernetes.io/target: "rke2-manager.techsecom.io"
        # external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
      hosts:
        core: harbor.ops.techsecom.io
        notary: notary.ops.techsecom.io
  proxy:
    httpProxy:
    httpsProxy:
    noProxy: 127.0.0.1,localhost,.local,.internal
    components:
    - core
    - jobservice
    - clair

  # Use RollingUpdate for HA - Recreate causes downtime
  updateStrategy:
    type: RollingUpdate

  persistence:
    enabled: true
    resourcePolicy: "keep"
    imageChartStorage:
      type: "s3"
      # S3 redirect disabled - all blob uploads go through Harbor registry
      # This is required for multi-replica registry deployments to avoid
      # "upload resumed at wrong offset" errors when session affinity fails
      # Trade-off: slightly higher latency, but reliable uploads
      disableredirect: true
      s3:
        bucket: harbor
        existingSecret: harbor-s3
        # External URL - CoreDNS rewrites this to internal HTTPS service for cluster traffic
        regionendpoint: "https://s3.ops.techsecom.io"
        # Reduced chunk size to 32MB to avoid upload timeout issues with S3
        # Smaller chunks = more reliable uploads at the cost of some overhead
        chunksize: "33554432"
        multipartcopychunksize: "33554432"
        multipartcopymaxconcurrency: 50
        multipartcopythresholdsize: "33554432"

  portal:
    replicas: 3
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi

  trivy:
    enabled: true
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 2Gi
    # Trivy vulnerability database update
    skipUpdate: false
    # Offline mode - use when not connected to internet
    offlineScan: false

  notary:
    enabled: true

  database:
    type: external
    external:
      host: "cnpg-cluster-rw.cnpg-system.svc.cluster.local"
      port: "5432"
      username: "harbor"
      existingSecret: "harbor-user-secret"

  redis:
    type: external
    external:
      sentinelMasterSet: "mymaster"
      addr: "redis-headless.harbor.svc.cluster.local:26379"
      # NOTE: Harbor helm chart has a bug with existingSecret for external Redis when Trivy is enabled
      # See: https://github.com/goharbor/harbor-helm/issues/2148
      # Using inline password until the upstream fix is available
      password: "Techsecoms-@rke2"

  cache:
    enabled: true
    expireHours: 24

  exporter:
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
    # oidc:
    #   enabled: true

    # # Configure components of the External Secrets Operator (ESO).
    # eso:
    #   enabled: true
    #   type: "vault"
    #   secretStoreName: "vault"
    #   # -- Value name in AWS ParameterStore, AWS SecretsManager or other Secret Store.
    #   secretName: "secret/harbor"
    #   # -- Role ARN for the ExternalSecretOperator to assume.

    #   gcpsm:
    #     projectID: "alphabet-123"
