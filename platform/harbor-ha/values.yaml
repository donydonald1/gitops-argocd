redis:
  fullnameOverride: redis
  image:
    registry: docker.io
    repository: bitnamilegacy/redis
  global:
    storageClass: longhorn
  auth:
    enabled: true
    existingSecret: redis-creds
    existingSecretPasswordKey: REDIS_PASSWORD
    sentinel: false
  # Performance tuning for Harbor Redis
  commonConfiguration: |-
    maxmemory 4gb
    maxmemory-policy volatile-lru
    appendonly yes
    save 900 1
    save 300 10
    save 60 10000
    tcp-keepalive 300
    activerehashing yes
    lazyfree-lazy-eviction yes
    lazyfree-lazy-expire yes
    io-threads 4
    io-threads-do-reads yes

  architecture: replication
  replica:
    replicaCount: 3
    disableCommands:
    - FLUSHDB
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2
        memory: 6Gi
    persistence:
      enabled: true
      storageClass: longhorn
      size: 60Gi
    terminationGracePeriodSeconds: 60
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 30
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 15
      timeoutSeconds: 15
      successThreshold: 1
      failureThreshold: 10

  kubectl:
    image:
      registry: docker.io
      repository: bitnamilegacy/kubectl

  sentinel:
    enabled: true
    image:
      registry: docker.io
      repository: bitnamilegacy/redis-sentinel
    masterSet: mymaster
    quorum: 2
    replicas: 3
    terminationGracePeriodSeconds: 60
    persistence:
      enabled: true
      storageClass: longhorn
      size: 60Gi
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 15
      failureThreshold: 20
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 10

  volumePermissions:
    enabled: true
    image:
      registry: docker.io
      repository: bitnamilegacy/os-shell

  metrics:
    enabled: true
    image:
      registry: docker.io
      repository: bitnamilegacy/redis-exporter
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
    startupProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 20
      successThreshold: 1
      failureThreshold: 10
    livenessProbe:
      enabled: true
      initialDelaySeconds: 90
      periodSeconds: 10
      timeoutSeconds: 10
      successThreshold: 1
      failureThreshold: 5
    readinessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 5
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 3

harbor:
  existingSecretAdminPassword: harbor
  existingSecretAdminPasswordKey: HARBOR_ADMIN_PASSWORD
  existingSecretSecretKey: harbor

  # Stakater Reloader - automatically reload pods when secrets change
  core:
    replicas: 3
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 4
        memory: 4Gi
    extraEnvVars:
    - name: OIDC_CLIENT_ID
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc-client-id
    - name: OIDC_CLIENT_SECRET
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc-client-secret
    - name: OIDC_ENDPOINT
      valueFrom:
        secretKeyRef:
          name: oidc-harbor
          key: oidc_issuer-url
    - name: CONFIG_OVERWRITE_JSON
      value: |
        {
          "auth_mode": "oidc_auth",
          "oidc_name": "OIDC",
          "oidc_endpoint": "$(OIDC_ENDPOINT)",
          "oidc_groups_claim": "groups",
          "oidc_admin_group": "superUsers",
          "oidc_client_id": "$(OIDC_CLIENT_ID)",
          "oidc_client_secret": "$(OIDC_CLIENT_SECRET)",
          "oidc_scope": "openid,email,groups,name",
          "oidc_verify_cert": "true",
          "oidc_auto_onboard": "true",
          "oidc_user_claim": "name"
        }

  jobservice:
    replicas: 3
    secret: ""
    podAnnotations:
      reloader.stakater.com/auto: "true"
    jobLoggers:
    - database
    # Increased workers for parallel job processing
    maxJobWorkers: 20
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 2Gi

  registry:
    replicas: 3
    secret: ""
    podAnnotations:
      reloader.stakater.com/auto: "true"
    credentials:
      # username is defined in existingSecret below, but this value also used by jobservice, so let's align it
      username: harbor_registry_user
      # we use harbor secret to consolidate all the secrets
      # Secret keys must be REGISTRY_PASSWD and REGISTRY_HTPASSWD. For key REGISTRY_PASSWD the value is the password.
      # For key REGISTRY_HTPASSWD the value is the string in the password file generated by htpasswd where the username
      # is harbor_registry_user and the encryption type is bcrypt.
      # For example: `htpasswd -bBc passwordfile harbor_registry_user harbor_registry_password`. The username must be harbor_registry_user!
      existingSecret: harbor
    registry:
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: 4
          memory: 8Gi
    controller:
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 2
          memory: 2Gi
    # trivy:
    #   # enabled the flag to enable Trivy scanner
    #   enabled: true
    gitHubToken: ""
  fullnameOverride: harbor
  externalURL: https://harbor.ops.techsecom.io
  ipFamily:
    # ipv6Enabled set to true if ipv6 is enabled in cluster, currently it affected the nginx related component
    ipv6:
      enabled: false
  expose:
    tls:
      enabled: true
      certSource: secret
      secret:
        secretName: harbor-tls
    ingress:
      className: nginx
      annotations:
        gethomepage.dev/enabled: "true"
        gethomepage.dev/group: Home
        gethomepage.dev/icon: harbor
        gethomepage.dev/name: Harbor
        gethomepage.dev/widget.type: harbor
        gethomepage.dev/widget.url: harbor.ops.techsecom.io
        gethomepage.dev/href: "https://harbor.ops.techsecom.io"
        gethomepage.dev/app: harbor-ingress
        ingress.kubernetes.io/ssl-redirect: "true"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        # Extended timeouts for large image uploads (e.g., 27GB Windows ISOs)
        nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "1800"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "1800"
        nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
        # external-dns.alpha.kubernetes.io/target: "rke2-manager.techsecom.io"
        # external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
      hosts:
        core: harbor.ops.techsecom.io
        notary: notary.ops.techsecom.io
  proxy:
    httpProxy:
    httpsProxy:
    noProxy: 127.0.0.1,localhost,.local,.internal
    components:
    - core
    - jobservice
    - clair

  # Use RollingUpdate for HA - Recreate causes downtime
  updateStrategy:
    type: RollingUpdate

  persistence:
    enabled: true
    resourcePolicy: "keep"
    imageChartStorage:
      type: "s3"
      # S3 redirect enabled - clients download blobs directly from MinIO
      # Split-horizon DNS (coredns-custom) rewrites s3.ops.techsecom.io to
      # minio-https.minio.svc.cluster.local for internal HTTPS access
      # External clients use Cloudflare -> ingress -> MinIO
      disableredirect: false
      s3:
        bucket: harbor
        existingSecret: harbor-s3
        # External URL - CoreDNS rewrites this to internal HTTPS service for cluster traffic
        regionendpoint: "https://s3.ops.techsecom.io"
        # Performance tuning: increase chunk size for faster large blob transfers
        chunksize: "104857600"
        multipartcopychunksize: "104857600"
        multipartcopymaxconcurrency: 100
        multipartcopythresholdsize: "104857600"

  portal:
    replicas: 3
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi

  trivy:
    enabled: true
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 2
        memory: 2Gi
    # Trivy vulnerability database update
    skipUpdate: false
    # Offline mode - use when not connected to internet
    offlineScan: false

  notary:
    enabled: true

  database:
    type: external
    external:
      host: "cnpg-cluster-rw.cnpg-system.svc.cluster.local"
      port: "5432"
      username: "harbor"
      existingSecret: "harbor-user-secret"

  redis:
    type: external
    external:
      sentinelMasterSet: "mymaster"
      addr: "redis-headless.harbor.svc.cluster.local:26379"
      # NOTE: Harbor helm chart has a bug with existingSecret for external Redis when Trivy is enabled
      # See: https://github.com/goharbor/harbor-helm/issues/2148
      # Using inline password until the upstream fix is available
      password: "Techsecoms-@rke2"

  cache:
    enabled: true
    expireHours: 24

  exporter:
    podAnnotations:
      reloader.stakater.com/auto: "true"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
    # oidc:
    #   enabled: true

    # # Configure components of the External Secrets Operator (ESO).
    # eso:
    #   enabled: true
    #   type: "vault"
    #   secretStoreName: "vault"
    #   # -- Value name in AWS ParameterStore, AWS SecretsManager or other Secret Store.
    #   secretName: "secret/harbor"
    #   # -- Role ARN for the ExternalSecretOperator to assume.

    #   gcpsm:
    #     projectID: "alphabet-123"
