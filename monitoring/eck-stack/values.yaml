########################################
# TLS Certificates Configuration (cert-manager)
########################################
certificates:
  enabled: true

  # Organization name for certificate subject
  organization: "Techsecoms Consulting Group LLC"

  # Certificate issuer configuration
  issuer:
    # Create a self-signed CA issuer (set to false if using external issuer like Vault)
    createSelfSigned: true
    # Name of the ClusterIssuer for the self-signed CA
    clusterIssuerName: elastic-system-cert-issuer
    # Name of the namespace Issuer (created from CA)
    issuerName: elastic-system-ca-issuer
    # Secret name for the root CA
    caSecretName: root-ca-secret

  # Elasticsearch certificate
  elasticsearch:
    enabled: true
    secretName: elasticsearch-es-cert
    # Additional DNS names for the certificate
    dnsNames:
    - elasticsearch-es-http
    - elasticsearch-es-http.elastic-system.svc
    - elasticsearch-es-http.elastic-system.svc.cluster.local
    # External hostnames (from ingress)
    externalDnsNames:
    - elasticsearch.ops.techsecom.io
    privateKey:
      algorithm: RSA
      size: 2048

  # Kibana certificate
  kibana:
    enabled: true
    secretName: kibana-tls-cert
    dnsNames:
    - kibana-kb-http
    - kibana-kb-http.elastic-system.svc
    - kibana-kb-http.elastic-system.svc.cluster.local
    externalDnsNames:
    - kibana.ops.techsecom.io
    privateKey:
      algorithm: RSA
      size: 2048

  # Logstash certificate
  logstash:
    enabled: true
    secretName: logstash-tls-cert
    dnsNames:
    - eck-stack-eck-logstash-ls-beats
    - eck-stack-eck-logstash-ls-beats.elastic-system.svc
    - eck-stack-eck-logstash-ls-beats.elastic-system.svc.cluster.local
    - eck-stack-eck-logstash-ls-http
    - eck-stack-eck-logstash-ls-http.elastic-system.svc
    - eck-stack-eck-logstash-ls-http.elastic-system.svc.cluster.local
    privateKey:
      algorithm: RSA
      size: 2048

########################################
# ILM (Index Lifecycle Management) Configuration
########################################
ilm:
  enabled: true
  image: curlimages/curl:latest

  # Elasticsearch connection settings
  elasticsearch:
    serviceName: elasticsearch-es-http
    port: 9200
    secretName: elasticsearch-es-elastic-user
    secretKey: elastic

  # Kibana connection settings
  kibana:
    serviceName: kibana-kb-http
    port: 5601

  # ILM Policies Configuration
  # TESTING VALUES: Using minutes/hours to observe data movement
  # For production, change to days (e.g., "2d", "7d", "30d")
  policies:
    # Audit Logs ILM Policy
    auditLogs:
      name: hot_warm_delete-audit-logs
      hot:
        rollover:
          maxAge: "5m" # Rollover after 5 minutes (testing)
          maxPrimaryShardSize: "1gb" # Or when shard reaches 1GB
          maxDocs: 10000 # Or when 10K docs (testing)
      warm:
        minAge: "10m" # Move to warm after 10 minutes
        replicas: 0 # Reduce replicas in warm
      cold:
        minAge: "30m" # Move to cold after 30 minutes
        replicas: 0 # No replicas in cold
      delete:
        minAge: "1h" # Delete after 1 hour total
        waitForSnapshot: "logs-snapshots" # Wait for SLM snapshot before delete

    # Container Logs ILM Policy
    containerLogs:
      name: hot_warm_delete-container-logs
      hot:
        rollover:
          maxAge: "5m" # Rollover after 5 minutes (testing)
          maxPrimaryShardSize: "1gb" # Or when shard reaches 1GB
          maxDocs: 10000 # Or when 10K docs (testing)
      warm:
        minAge: "10m" # Move to warm after 10 minutes
        replicas: 0 # Reduce replicas in warm
      cold:
        minAge: "30m" # Move to cold after 30 minutes
        replicas: 0 # No replicas in cold
      delete:
        minAge: "1h" # Delete after 1 hour total
        waitForSnapshot: "logs-snapshots" # Wait for SLM snapshot before delete

  # Index Templates Configuration
  indexTemplates:
    auditLogs:
      shards: 1
      replicas: 1
      refreshInterval: "5s"
    containerLogs:
      shards: 1
      replicas: 1
      refreshInterval: "5s"

########################################
# S3/MinIO Snapshot Configuration
########################################
snapshots:
  enabled: true
  # S3/MinIO repository settings
  s3:
    endpoint: "http://minio.minio.svc:9000"
    bucket: "elasticsearch-snapshots"
    basePath: "snapshots"
    # Secret containing S3 credentials (will be created in elastic-system namespace)
    secretName: elasticsearch-s3-credentials
    accessKey: elasticsearch
    # The secret key is fetched from minio-users secret
  # Snapshot Lifecycle Management (SLM) policy
  slm:
    name: "logs-snapshots"
    schedule: "0 */30 * * * ?"  # Every 30 minutes (for testing)
    # schedule: "0 0 1 * * ?"   # Production: Daily at 1 AM
    retention:
      expireAfter: "7d"         # Keep snapshots for 7 days
      minCount: 2               # Keep at least 2 snapshots
      maxCount: 50              # Keep at most 50 snapshots

########################################
# Kibana Spaces, Roles, and Role Mappings
########################################
kibana:
  spaces:
    enabled: false
    list:
    - id: "logs"
      name: "Logs"
      description: "Space for log analysis"
      disabledFeatures: []
    - id: "security"
      name: "Security"
      description: "Space for security monitoring"
      disabledFeatures: []

  roles:
    enabled: false
    list:
    - name: "logs_reader"
      definition:
        cluster: []
        indices:
        - names:
          - "container-logs-*"
          - "audit-logs-*"
          privileges:
          - "read"
          - "view_index_metadata"
        applications:
        - application: "kibana-.kibana"
          privileges:
          - "feature_discover.read"
          - "feature_dashboard.read"
          resources:
          - "space:logs"
    - name: "logs_admin"
      definition:
        cluster:
        - "monitor"
        indices:
        - names:
          - "container-logs-*"
          - "audit-logs-*"
          privileges:
          - "all"
        applications:
        - application: "kibana-.kibana"
          privileges:
          - "feature_discover.all"
          - "feature_dashboard.all"
          - "feature_canvas.all"
          - "feature_maps.all"
          - "feature_visualize.all"
          resources:
          - "space:logs"

  roleMappings:
    enabled: false
    list:
    - name: "logs_reader_mapping"
      definition:
        enabled: true
        roles:
        - "logs_reader"
        rules:
          field:
            groups: "cn=log-readers,ou=groups,dc=Techsecom,dc=com"
        metadata:
          description: "Maps LDAP log-readers group to logs_reader role"
    - name: "logs_admin_mapping"
      definition:
        enabled: true
        roles:
        - "logs_admin"
        rules:
          field:
            groups: "cn=log-admins,ou=groups,dc=Techsecom,dc=com"
        metadata:
          description: "Maps LDAP log-admins group to logs_admin role"

########################################
# ECK Stack Configuration
########################################
eck-stack:
  elasticPassword: "Techsecoms-@rke2"
  eck-elasticsearch:
    version: 8.19.9
    fullnameOverride: elasticsearch
    auth:
      # disableElasticUser: true
      roles:
      - secretName: elasticsearch-admin
    # S3 repository credentials for snapshots to MinIO
    secureSettings:
    - secretName: elasticsearch-s3-credentials
    ingress:
      enabled: true
      className: nginx
      annotations:
        my: annotation
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
      labels:
        my: label
      pathType: Prefix
      hosts:
      - host: "elasticsearch.ops.techsecom.io"
        path: "/"
    http:
      tls:
        selfSignedCertificate:
          disabled: true
        certificate:
          secretName: elasticsearch-es-cert
    nodeSets:
    - name: masters
      count: 3
      config:
        node.roles: [ "master", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              limits:
                memory: 2Gi
                cpu: 2
              requests:
                memory: 2Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 15Gi
          storageClassName: nfs-csi

    - name: hot
      count: 3
      config:
        node.roles: [ "data_hot", "data_content", "ingest", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
        node.attr.data: hot
        node.attr.box_type: hot
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              # Increased from 3Gi to 6Gi to prevent OOMKilled under heavy indexing
              limits:
                memory: 6Gi
                cpu: 2
              requests:
                memory: 6Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 15Gi
          storageClassName: nfs-csi
    - name: warm
      count: 3
      config:
        node.roles: [ "data_warm", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
        node.attr.data: warm
        node.attr.box_type: warm
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              # Increased from 3Gi to 6Gi to handle shard recovery during ILM migration
              limits:
                memory: 6Gi
                cpu: 2
              requests:
                memory: 6Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          storageClassName: nfs-csi
    - name: cold
      count: 3
      config:
        node.roles: [ "data_cold", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.security.enrollment.enabled: true
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        # xpack.ml.enabled: false
        node.attr.data: cold
        node.attr.box_type: cold
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              limits:
                memory: 3Gi
                cpu: 2
              requests:
                memory: 3Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
          storageClassName: nfs-csi
  eck-kibana:
    enabled: true
    version: 8.19.9
    # Name of the Kibana instance.
    #
    fullnameOverride: kibana

    spec:
      count: 1
      elasticsearchRef:
        name: elasticsearch

      config:
        elasticsearch.requestHeadersWhitelist:
        - authorization
        server.publicBaseUrl: https://kibana.ops.techsecom.io
      http:
        tls:
          certificate:
            secretName: elasticsearch-es-cert # Use the cert-manager generated secret

      podTemplate:
        metadata:
          labels:
            scrape: kob-r1-lab-kb
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: kibana
            resources:
              # CPU must be multiples of 2 due to SMT alignment on your nodes
              requests:
                memory: 1Gi
                cpu: 2
              limits:
                memory: 2Gi
                cpu: 2
            env:
            - name: NODE_OPTIONS
              value: "--max-old-space-size=2048"
            - name: SERVER_PUBLICBASEURL
              value: "https://kibana.ops.techsecom.io"

            volumeMounts:
            - name: certs
              mountPath: /usr/share/kibana/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert

    ingress:
      enabled: true
      className: nginx
      annotations:
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
        nginx.ingress.kubernetes.io/proxy-body-size: 20m
        cert-manager.io/cluster-issuer: letsencrypt-prod
      pathType: Prefix
      hosts:
      - host: "kibana.ops.techsecom.io"
        path: "/"
      tls:
        enabled: true
        secretName: kibana-tls
  eck-beats:
    enabled: true
    version: 8.19.9
    spec:
      type: filebeat
      # daemonSet:
      daemonSet:
        podTemplate:
          # Metadata so that Metricbeat can scrape metrics from these pods
          metadata:
            labels:
              scrape: kob-r1-lab-fb
            annotations:
              # Disable Stakater Reloader - ECK manages its own config updates
              reloader.stakater.com/auto: "false"
          spec:
            serviceAccountName: filebeat
            automountServiceAccountToken: true
            terminationGracePeriodSeconds: 30
            dnsPolicy: ClusterFirstWithHostNet
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
              operator: Exists
            # hostNetwork: true
            containers:
            - name: filebeat
              resources:
                # CPU must be multiples of 2 due to SMT alignment on your nodes
                limits:
                  memory: 8Gi
                  cpu: 4
                requests:
                  cpu: 4
                  memory: 4Gi
              securityContext:
                runAsUser: 0
              volumeMounts:
              - name: varlogcontainers
                mountPath: /var/log/containers
              - name: auditlog
                mountPath: /var/log/rancher
              - name: varlogpods
                mountPath: /var/log/pods
              env:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
            volumes:
            - name: varlogcontainers
              hostPath:
                path: /var/log/containers
            - name: varlogpods
              hostPath:
                path: /var/log/pods
            - name: auditlog
              hostPath:
                path: /var/log/rancher
            - name: data
              emptydir: {}
            - name: beat-data
              emptydir: {}
      config:
        http.enabled: true
        http.host: 0.0.0.0
        http.port: 5066
        # Memory optimization settings
        queue.mem:
          events: 1024
          flush.min_events: 256
          flush.timeout: 5s
        # Limit harvesters to reduce memory usage
        filebeat.registry.flush: 30s
        # Close harvesters for idle files
        close_inactive: 5m
        close_removed: true
        close_renamed: true
        # Limit concurrent harvesters
        harvester_limit: 50
        enabled: true
        filebeat.autodiscover:
          providers:
          - type: kubernetes
            hints.enabled: true
            include_labels: [ "app.kubernetes.io/name" ]
            labels.dedot: true
            node: ${NODE_NAME}
            hints.default_config:
              type: container
              paths:
              - /var/log/containers/*-${data.container.id}.log
              exclude_lines: [ "^\\s+[\\-`('.|_]" ]
              # Removed per-input add_kubernetes_metadata - using global processor instead
        filebeat.inputs:
        - type: filestream
          id: audit-logs
          enabled: true
          # max_bytes: "1048576"
          paths:
          - /var/log/rancher/*.log
          scan_frequency: 10s
          json.keys_under_root: true
          json.add_error_key: true
          tags: [ "audit" ]
          tail_files: true
          parsers:
          - multiline:
              type: pattern
              pattern: "^[0-9]{4}-[0-9]{2}-[0-9]{2}"
              negate: false
              match: after
              max_lines: 1000
              timeout: 5s
        filebeat.config.modules:
          path: ${path.config}/modules.d/*.yml
          reload.enabled: true
          reload.period: 10s
        setup.template.settings:
          index.number_of_shards: 1
        processors:
        - add_host_metadata: {}
        - add_cloud_metadata: {}
        # Global add_kubernetes_metadata - more efficient than per-input
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
        output.logstash:
          # This needs to be {{logstash-name}}-ls-beats:5044
          hosts: [ "eck-stack-eck-logstash-ls-beats.elastic-system.svc:5044" ]
      # deployment: null

  eck-logstash:
    enabled: true
    version: 8.19.9
    config:
      log.level: info
      queue.type: persisted
      path.queue: /usr/share/logstash/dp
      dead_letter_queue.max_bytes: 10gb
      # pipeline.buffer.type: heap

    podTemplate:
      metadata:
        annotations:
          # Disable Stakater Reloader - ECK manages its own config updates
          reloader.stakater.com/auto: "false"
      spec:
        # Override init container resources to fix SMT alignment error
        # Your nodes require CPU requests to be multiples of 2 (SMT cores per physical core)
        initContainers:
        - name: logstash-internal-init-config
          resources:
            requests:
              cpu: 2
              memory: 128Mi
            limits:
              cpu: 2
              memory: 256Mi
        - name: elastic-internal-init-keystore
          resources:
            requests:
              cpu: 2
              memory: 1Gi
            limits:
              cpu: 2
              memory: 2Gi
        containers:
        - name: logstash
          resources:
            # Memory limit should be ~2x heap size to account for off-heap usage
            # JVM heap is 4GB, limit is 8GB for complex filters with JSON parsing
            requests:
              memory: 8Gi
              cpu: 2
            limits:
              memory: 8Gi
              cpu: 2
          env:
          - name: ES_USER
            value: "elastic"
          - name: ES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: elasticsearch-es-elastic-user
                key: elastic
          # JVM options: 4GB heap with maxOrder=14 to handle large Filebeat batches
          - name: LS_JAVA_OPTS
            value: "-Xms4g -Xmx4g -Dio.netty.allocator.maxOrder=14"
          volumeMounts:
          - mountPath: /usr/share/logstash/pq
            name: pq
            readOnly: false
          - mountPath: /usr/share/logstash/dlq
            name: dlq
            readOnly: false
          - mountPath: /etc/logstash/config/certs
            name: certs
            readOnly: true
        volumes:
        - name: certs
          secret:
            secretName: elasticsearch-es-cert
    pipelines:
    - pipeline.id: main
      dead_letter_queue.enable: true
      path.dead_letter_queue: /usr/share/logstash/dlq
      config.reload.automatic: true
      config.reload.interval: 30s
      dead_letter_queue.storage_policy: drop_older
      dead_letter_queue.max_bytes: 5gb
      dead_letter_queue.retain.age: 10m
      pipeline.ecs_compatibility: disabled
      config.string: |
        input {
          beats {
            port => 5044
          }
        }

        filter {
          #===========================================
          # AUDIT LOGS PROCESSING
          #===========================================
          if "audit" in [tags] {
            # Parse the JSON audit log message
            json {
              source => "message"
              target => "audit_raw"
              skip_on_invalid_json => true
            }

            # Drop RequestReceived stage - only keep ResponseStarted/ResponseComplete
            # This eliminates duplicate entries for each API call
            if [audit_raw][stage] == "RequestReceived" {
              drop { }
            }

            if [audit_raw] {
              # Extract and transform audit fields for human readability
              ruby {
                code => '
                  audit = event.get("audit_raw")
                  return unless audit

                  # Core identification
                  event.set("id", audit["auditID"]) if audit["auditID"]

                  # Action info - human readable
                  verb = audit["verb"] || "unknown"
                  event.set("action", verb.upcase)

                  # Categorize for filtering
                  case verb
                  when "create" then event.set("action_type", "WRITE")
                  when "delete" then event.set("action_type", "DELETE")
                  when "update", "patch" then event.set("action_type", "WRITE")
                  when "get", "list", "watch" then event.set("action_type", "READ")
                  else event.set("action_type", "OTHER")
                  end

                  # User info - clean up service account names
                  if audit["user"]
                    username = audit["user"]["username"] || "unknown"
                    # Simplify service account names for readability
                    if username.start_with?("system:serviceaccount:")
                      parts = username.gsub("system:serviceaccount:", "").split(":")
                      event.set("user", "sa:#{parts.last}")
                      event.set("user_namespace", parts.first) if parts.length > 1
                      event.set("user_type", "serviceaccount")
                    elsif username.start_with?("system:")
                      event.set("user", username.gsub("system:", ""))
                      event.set("user_type", "system")
                    else
                      event.set("user", username)
                      event.set("user_type", "user")
                    end
                    event.set("user_full", username)

                    # User groups
                    groups = audit["user"]["groups"]
                    if groups && groups.is_a?(Array)
                      event.set("user_groups", groups.reject { |g| g.start_with?("system:") }.join(", "))
                      event.set("is_admin", groups.include?("system:masters"))
                    end
                  end

                  # Resource info
                  if audit["objectRef"]
                    ref = audit["objectRef"]
                    resource = ref["resource"] || "unknown"
                    name = ref["name"]
                    ns = ref["namespace"]
                    api = ref["apiVersion"]

                    event.set("resource", resource)
                    event.set("resource_name", name) if name
                    event.set("namespace", ns) if ns
                    event.set("api_version", api) if api

                    # Build human-readable target
                    target_parts = []
                    target_parts << ns if ns
                    target_parts << "#{resource}/#{name}" if name
                    target_parts << resource unless name
                    event.set("target", target_parts.join("/"))
                  end

                  # Request details
                  uri = audit["requestURI"]
                  event.set("request_uri", uri) if uri

                  # Audit stage (RequestReceived vs ResponseComplete)
                  stage = audit["stage"]
                  event.set("stage", stage) if stage

                  # Response status
                  status_text = nil
                  if audit["responseStatus"]
                    code = audit["responseStatus"]["code"]
                    if code
                      event.set("status_code", code.to_i)
                      event.set("success", code.to_i < 400)
                      # Human readable status
                      status_text = case code.to_i
                      when 101 then "Switching Protocols"
                      when 200 then "OK"
                      when 201 then "Created"
                      when 204 then "No Content"
                      when 400 then "Bad Request"
                      when 401 then "Unauthorized"
                      when 403 then "Forbidden"
                      when 404 then "Not Found"
                      when 409 then "Conflict"
                      when 422 then "Invalid"
                      when 429 then "Rate Limited"
                      when 200..299 then "OK"
                      when 500..599 then "Server Error"
                      else code.to_s
                      end
                      event.set("status", status_text)
                    end
                  end

                  # Source IP - clean up localhost
                  if audit["sourceIPs"] && audit["sourceIPs"].is_a?(Array)
                    ip = audit["sourceIPs"].first
                    ip = "localhost" if ip == "::1" || ip == "127.0.0.1"
                    event.set("source_ip", ip)
                  end

                  # Build human-readable summary message
                  user_display = event.get("user") || "unknown"
                  action_display = verb.upcase
                  target_display = event.get("target") || event.get("request_uri") || "unknown"
                  status_display = status_text || "-"

                  summary = "#{user_display} #{action_display} #{target_display} -> #{status_display}"
                  event.set("summary", summary)
                '
              }

              # Remove the raw audit object and other verbose fields
              mutate {
                remove_field => [
                  "audit_raw",
                  "message",
                  "ecs",
                  "agent",
                  "input",
                  "log",
                  "tags"
                ]
              }
            }

            mutate {
              add_field => { "[@metadata][target_index]" => "audit-logs" }
            }
          }

          #===========================================
          # CONTAINER LOGS PROCESSING
          #===========================================
          else {
            # Extract Kubernetes metadata using Ruby (safe field access)
            ruby {
              code => '
                k8s = event.get("kubernetes")
                if k8s
                  event.set("namespace", k8s["namespace"]) if k8s["namespace"]
                  event.set("pod", k8s.dig("pod", "name")) if k8s.dig("pod", "name")
                  event.set("container", k8s.dig("container", "name")) if k8s.dig("container", "name")
                  event.set("node", k8s.dig("node", "name")) if k8s.dig("node", "name")

                  # Extract app name
                  labels = k8s["labels"] || {}
                  app = labels["app"] || labels["app_kubernetes_io/name"] || k8s.dig("container", "name")
                  event.set("app", app) if app
                end
              '
            }

            # Try to parse JSON logs (many apps output structured JSON)
            if [message] =~ /^\s*\{/ {
              json {
                source => "message"
                target => "json_log"
                skip_on_invalid_json => true
              }

              if [json_log] {
                # Extract common JSON log fields using Ruby
                ruby {
                  code => '
                    jl = event.get("json_log")
                    if jl
                      # Log level
                      level = jl["level"] || jl["severity"] || jl["lvl"]
                      event.set("log_level", level.to_s.upcase) if level

                      # Message
                      msg = jl["msg"] || jl["message"]
                      event.set("log_message", msg) if msg

                      # Error
                      err = jl["error"] || jl["err"]
                      event.set("error_message", err) if err

                      # Stack trace
                      st = jl["stack_trace"] || jl["stacktrace"]
                      event.set("stack_trace", st) if st
                    end
                  '
                }
              }
            }

            # Extract log level from message if not already set
            if ![log_level] {
              grok {
                match => { "message" => "(?i)(?<log_level>FATAL|ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE)" }
                tag_on_failure => []
              }
              if [log_level] {
                mutate { uppercase => [ "log_level" ] }
              }
            }

            # Normalize WARNING to WARN and set default
            ruby {
              code => '
                level = event.get("log_level")
                if level
                  level = level.to_s.upcase
                  level = "WARN" if level == "WARNING"
                  event.set("log_level", level)
                else
                  event.set("log_level", "INFO")
                end

                # Set severity
                if level == "FATAL" || level == "ERROR"
                  event.set("log_severity", "error")
                elsif level == "WARN"
                  event.set("log_severity", "warning")
                else
                  event.set("log_severity", "info")
                end
              '
            }

            # Clean up - remove verbose nested fields
            mutate {
              remove_field => [
                "[kubernetes][namespace_labels]",
                "[kubernetes][namespace_uid]",
                "[kubernetes][node][labels]",
                "[kubernetes][node][uid]",
                "[kubernetes][node][hostname]",
                "[kubernetes][pod][ip]",
                "[kubernetes][pod][uid]",
                "[kubernetes][labels]",
                "[kubernetes][replicaset]",
                "[kubernetes][statefulset]",
                "[kubernetes][daemonset]",
                "[kubernetes][deployment]",
                "[container][runtime]",
                "[container][id]",
                "[container][image]",
                "[event]",
                "[service]",
                "[ecs]",
                "[agent]",
                "[input]",
                "[log][offset]",
                "[log][file]",
                "json_log",
                "tags"
              ]
            }

            mutate {
              add_field => { "[@metadata][target_index]" => "container-logs" }
            }
          }

          # Remove common unnecessary fields from all logs
          mutate {
            remove_field => [ "@version", "host" ]
          }
        }

        output {
          if [@metadata][target_index] == "container-logs" {
            elasticsearch {
              hosts => [ "https://elasticsearch-es-http.elastic-system.svc:9200" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              ilm_rollover_alias => "container-logs"
              ilm_pattern => "000001"
              ilm_enabled => true
              ilm_policy => "hot_warm_delete-container-logs"
              # Disable Logstash template management - ILM setup job manages templates
              # This prevents conflicts with data tier routing settings
              manage_template => false
              data_stream => auto
              ssl_enabled => true
              ssl_certificate_authorities => ["/etc/logstash/config/certs/ca.crt"]
            }
          } else if [@metadata][target_index] == "audit-logs" {
            elasticsearch {
              hosts => [ "https://elasticsearch-es-http.elastic-system.svc:9200" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              ilm_enabled => true
              ilm_rollover_alias => "audit-logs"
              ilm_pattern => "000001"
              ilm_policy => "hot_warm_delete-audit-logs"
              # Disable Logstash template management - ILM setup job manages templates
              # This prevents conflicts with data tier routing settings
              manage_template => false
              ssl_enabled => true
              data_stream => auto
              ssl_certificate_authorities => ["/etc/logstash/config/certs/ca.crt"]
            }
          }
        }
    volumeClaimTemplates:
    - metadata:
        name: pq
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: nfs-csi
        resources:
          requests:
            storage: 15Gi
    - metadata:
        name: dlq
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: nfs-csi
        resources:
          requests:
            storage: 20Gi
    - metadata:
        name: logstash-data
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: nfs-csi
        resources:
          requests:
            storage: 2Gi

    elasticsearchRefs:
    - clusterName: eck
      name: elasticsearch
    secureSettings:
    - secretName: elasticsearch-es-cert
    services:
    - name: beats
      service:
        spec:
          type: ClusterIP
          ports:
          - port: 5044
            name: "filebeat"
            protocol: TCP
            targetPort: 5044
