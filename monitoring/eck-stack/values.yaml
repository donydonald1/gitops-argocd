########################################
# TLS Certificates Configuration (cert-manager)
########################################
certificates:
  enabled: true

  # Organization name for certificate subject
  organization: "Techsecoms Consulting Group LLC"

  # Certificate issuer configuration
  issuer:
    # Create a self-signed CA issuer (set to false if using external issuer like Vault)
    createSelfSigned: true
    # Name of the ClusterIssuer for the self-signed CA
    clusterIssuerName: elastic-system-cert-issuer
    # Name of the namespace Issuer (created from CA)
    issuerName: elastic-system-ca-issuer
    # Secret name for the root CA
    caSecretName: root-ca-secret

  # Elasticsearch certificate
  elasticsearch:
    enabled: true
    secretName: elasticsearch-es-cert
    # Additional DNS names for the certificate
    dnsNames:
    - elasticsearch-es-http
    - elasticsearch-es-http.elastic-system.svc
    - elasticsearch-es-http.elastic-system.svc.cluster.local
    # External hostnames (from ingress)
    externalDnsNames:
    - elasticsearch.ops.techsecom.io
    privateKey:
      algorithm: RSA
      size: 2048

  # Kibana certificate
  kibana:
    enabled: true
    secretName: kibana-tls-cert
    dnsNames:
    - kibana-kb-http
    - kibana-kb-http.elastic-system.svc
    - kibana-kb-http.elastic-system.svc.cluster.local
    externalDnsNames:
    - kibana.ops.techsecom.io
    privateKey:
      algorithm: RSA
      size: 2048

  # Logstash certificate
  logstash:
    enabled: true
    secretName: logstash-tls-cert
    dnsNames:
    - eck-stack-eck-logstash-ls-beats
    - eck-stack-eck-logstash-ls-beats.elastic-system.svc
    - eck-stack-eck-logstash-ls-beats.elastic-system.svc.cluster.local
    - eck-stack-eck-logstash-ls-http
    - eck-stack-eck-logstash-ls-http.elastic-system.svc
    - eck-stack-eck-logstash-ls-http.elastic-system.svc.cluster.local
    privateKey:
      algorithm: RSA
      size: 2048

########################################
# ILM (Index Lifecycle Management) Configuration
########################################
ilm:
  enabled: true
  image: curlimages/curl:latest

  # Elasticsearch connection settings
  elasticsearch:
    serviceName: elasticsearch-es-http
    port: 9200
    secretName: elasticsearch-es-elastic-user
    secretKey: elastic

  # Kibana connection settings
  kibana:
    serviceName: kibana-kb-http
    port: 5601

  # ILM Policies Configuration
  # TESTING VALUES: Using minutes/hours to observe data movement
  # For production, change to days (e.g., "2d", "7d", "30d")
  policies:
    # Audit Logs ILM Policy
    auditLogs:
      name: hot_warm_delete-audit-logs
      hot:
        rollover:
          maxAge: "5m" # Rollover after 5 minutes (testing)
          maxPrimaryShardSize: "1gb" # Or when shard reaches 1GB
          maxDocs: 10000 # Or when 10K docs (testing)
      warm:
        minAge: "10m" # Move to warm after 10 minutes
        replicas: 0 # Reduce replicas in warm
      cold:
        minAge: "30m" # Move to cold after 30 minutes
        replicas: 0 # No replicas in cold
      delete:
        minAge: "1h" # Delete after 1 hour total
        waitForSnapshot: "logs-snapshots" # Wait for SLM snapshot before delete

    # Container Logs ILM Policy
    containerLogs:
      name: hot_warm_delete-container-logs
      hot:
        rollover:
          maxAge: "5m" # Rollover after 5 minutes (testing)
          maxPrimaryShardSize: "1gb" # Or when shard reaches 1GB
          maxDocs: 10000 # Or when 10K docs (testing)
      warm:
        minAge: "10m" # Move to warm after 10 minutes
        replicas: 0 # Reduce replicas in warm
      cold:
        minAge: "30m" # Move to cold after 30 minutes
        replicas: 0 # No replicas in cold
      delete:
        minAge: "1h" # Delete after 1 hour total
        waitForSnapshot: "logs-snapshots" # Wait for SLM snapshot before delete

  # Index Templates Configuration
  indexTemplates:
    auditLogs:
      shards: 1
      replicas: 1
      refreshInterval: "5s"
    containerLogs:
      shards: 1
      replicas: 1
      refreshInterval: "5s"

########################################
# S3/MinIO Snapshot Configuration
########################################
snapshots:
  enabled: true
  # S3/MinIO repository settings
  s3:
    endpoint: "http://minio.minio.svc:9000"
    bucket: "elasticsearch-snapshots"
    basePath: "snapshots"
    # Secret containing S3 credentials (will be created in elastic-system namespace)
    secretName: elasticsearch-s3-credentials
    accessKey: elasticsearch
    # The secret key is fetched from minio-users secret
  # Snapshot Lifecycle Management (SLM) policy
  slm:
    name: "logs-snapshots"
    schedule: "0 */30 * * * ?"  # Every 30 minutes (for testing)
    # schedule: "0 0 1 * * ?"   # Production: Daily at 1 AM
    retention:
      expireAfter: "7d"         # Keep snapshots for 7 days
      minCount: 2               # Keep at least 2 snapshots
      maxCount: 50              # Keep at most 50 snapshots

########################################
# Kibana Spaces, Roles, and Role Mappings
########################################
kibana:
  spaces:
    enabled: false
    list:
    - id: "logs"
      name: "Logs"
      description: "Space for log analysis"
      disabledFeatures: []
    - id: "security"
      name: "Security"
      description: "Space for security monitoring"
      disabledFeatures: []

  roles:
    enabled: false
    list:
    - name: "logs_reader"
      definition:
        cluster: []
        indices:
        - names:
          - "container-logs-*"
          - "audit-logs-*"
          privileges:
          - "read"
          - "view_index_metadata"
        applications:
        - application: "kibana-.kibana"
          privileges:
          - "feature_discover.read"
          - "feature_dashboard.read"
          resources:
          - "space:logs"
    - name: "logs_admin"
      definition:
        cluster:
        - "monitor"
        indices:
        - names:
          - "container-logs-*"
          - "audit-logs-*"
          privileges:
          - "all"
        applications:
        - application: "kibana-.kibana"
          privileges:
          - "feature_discover.all"
          - "feature_dashboard.all"
          - "feature_canvas.all"
          - "feature_maps.all"
          - "feature_visualize.all"
          resources:
          - "space:logs"

  roleMappings:
    enabled: false
    list:
    - name: "logs_reader_mapping"
      definition:
        enabled: true
        roles:
        - "logs_reader"
        rules:
          field:
            groups: "cn=log-readers,ou=groups,dc=Techsecom,dc=com"
        metadata:
          description: "Maps LDAP log-readers group to logs_reader role"
    - name: "logs_admin_mapping"
      definition:
        enabled: true
        roles:
        - "logs_admin"
        rules:
          field:
            groups: "cn=log-admins,ou=groups,dc=Techsecom,dc=com"
        metadata:
          description: "Maps LDAP log-admins group to logs_admin role"

########################################
# ECK Stack Configuration
########################################
eck-stack:
  elasticPassword: "Techsecoms-@rke2"
  eck-elasticsearch:
    version: 8.19.9
    fullnameOverride: elasticsearch
    auth:
      # disableElasticUser: true
      roles:
      - secretName: elasticsearch-admin
    # S3 repository credentials for snapshots to MinIO
    secureSettings:
    - secretName: elasticsearch-s3-credentials
    ingress:
      enabled: true
      className: nginx
      annotations:
        my: annotation
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
      labels:
        my: label
      pathType: Prefix
      hosts:
      - host: "elasticsearch.ops.techsecom.io"
        path: "/"
    http:
      tls:
        selfSignedCertificate:
          disabled: true
        certificate:
          secretName: elasticsearch-es-cert
    nodeSets:
    - name: masters
      count: 3
      config:
        node.roles: [ "master", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              limits:
                memory: 2Gi
                cpu: 2
              requests:
                memory: 2Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 15Gi
          storageClassName: longhorn

    - name: hot
      count: 3
      config:
        node.roles: [ "data_hot", "data_content", "ingest", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
        node.attr.data: hot
        node.attr.box_type: hot
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              # Increased from 3Gi to 6Gi to prevent OOMKilled under heavy indexing
              limits:
                memory: 6Gi
                cpu: 2
              requests:
                memory: 6Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 15Gi
          storageClassName: longhorn
    - name: warm
      count: 3
      config:
        node.roles: [ "data_warm", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        xpack.security.enrollment.enabled: true
        # xpack.ml.enabled: false
        node.attr.data: warm
        node.attr.box_type: warm
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              limits:
                memory: 3Gi
                cpu: 2
              requests:
                memory: 3Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          storageClassName: longhorn
    - name: cold
      count: 3
      config:
        node.roles: [ "data_cold", "remote_cluster_client" ]
        node.store.allow_mmap: false
        # xpack.security.enabled: true
        # xpack.security.transport.ssl.enabled: true
        # xpack.security.http.ssl.enabled: true
        # xpack.monitoring.enabled: false
        xpack.security.enrollment.enabled: true
        xpack.graph.enabled: false
        xpack.watcher.enabled: false
        # xpack.ml.enabled: false
        node.attr.data: cold
        node.attr.box_type: cold
      podTemplate:
        metadata:
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: elasticsearch
            resources:
              # ECK requires memory request = limit for ResourcesAwareManagement
              limits:
                memory: 3Gi
                cpu: 2
              requests:
                memory: 3Gi
                cpu: 2
            volumeMounts:
            - name: certs
              mountPath: /usr/share/elasticsearch/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: elasticsearch-data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
          storageClassName: longhorn
  eck-kibana:
    enabled: true
    version: 8.19.9
    # Name of the Kibana instance.
    #
    fullnameOverride: kibana

    spec:
      count: 1
      elasticsearchRef:
        name: elasticsearch

      config:
        elasticsearch.requestHeadersWhitelist:
        - authorization
        server.publicBaseUrl: https://kibana.ops.techsecom.io
      http:
        tls:
          certificate:
            secretName: elasticsearch-es-cert # Use the cert-manager generated secret

      podTemplate:
        metadata:
          labels:
            scrape: kob-r1-lab-kb
          annotations:
            # Disable Stakater Reloader - ECK manages its own config updates
            reloader.stakater.com/auto: "false"
        spec:
          containers:
          - name: kibana
            resources:
              # CPU must be multiples of 2 due to SMT alignment on your nodes
              requests:
                memory: 1Gi
                cpu: 2
              limits:
                memory: 2Gi
                cpu: 2
            env:
            - name: NODE_OPTIONS
              value: "--max-old-space-size=2048"
            - name: SERVER_PUBLICBASEURL
              value: "https://kibana.ops.techsecom.io"

            volumeMounts:
            - name: certs
              mountPath: /usr/share/kibana/config/certs
              readOnly: true
          volumes:
          - name: certs
            secret:
              secretName: elasticsearch-es-cert

    ingress:
      enabled: true
      className: nginx
      annotations:
        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
        nginx.ingress.kubernetes.io/proxy-body-size: 20m
        cert-manager.io/cluster-issuer: letsencrypt-prod
      pathType: Prefix
      hosts:
      - host: "kibana.ops.techsecom.io"
        path: "/"
      tls:
        enabled: true
        secretName: kibana-tls
  eck-beats:
    enabled: true
    version: 8.19.9
    spec:
      type: filebeat
      # daemonSet:
      daemonSet:
        podTemplate:
          # Metadata so that Metricbeat can scrape metrics from these pods
          metadata:
            labels:
              scrape: kob-r1-lab-fb
            annotations:
              # Disable Stakater Reloader - ECK manages its own config updates
              reloader.stakater.com/auto: "false"
          spec:
            serviceAccountName: filebeat
            automountServiceAccountToken: true
            terminationGracePeriodSeconds: 30
            dnsPolicy: ClusterFirstWithHostNet
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
              operator: Exists
            # hostNetwork: true
            containers:
            - name: filebeat
              resources:
                # CPU must be multiples of 2 due to SMT alignment on your nodes
                limits:
                  memory: 8Gi
                  cpu: 4
                requests:
                  cpu: 4
                  memory: 4Gi
              securityContext:
                runAsUser: 0
              volumeMounts:
              - name: varlogcontainers
                mountPath: /var/log/containers
              - name: auditlog
                mountPath: /var/log/rancher
              - name: varlogpods
                mountPath: /var/log/pods
              env:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
            volumes:
            - name: varlogcontainers
              hostPath:
                path: /var/log/containers
            - name: varlogpods
              hostPath:
                path: /var/log/pods
            - name: auditlog
              hostPath:
                path: /var/log/rancher
            - name: data
              emptydir: {}
            - name: beat-data
              emptydir: {}
      config:
        http.enabled: true
        http.host: 0.0.0.0
        http.port: 5066
        # queue.mem:
        #   events: 2048
        #   flush.min_events: 50
        #   flush.timeout: 5s
        enabled: true
        filebeat.autodiscover:
          providers:
          - type: kubernetes
            hints.enabled: true
            include_labels: [ "app.kubernetes.io/name" ]
            labels.dedot: true
            node: ${NODE_NAME}
            hints.default_config:
              type: container
              paths:
              - /var/log/containers/*-${data.container.id}.log
              exclude_lines: [ "^\\s+[\\-`('.|_]" ]
              processors:
              - add_kubernetes_metadata:
                  host: ${NODE_NAME}
                  matchers:
                  - logs_path:
                      logs_path: "/var/log/containers/"
        filebeat.inputs:
        - type: filestream
          id: audit-logs
          enabled: true
          # max_bytes: "1048576"
          paths:
          - /var/log/rancher/*.log
          scan_frequency: 10s
          json.keys_under_root: true
          json.add_error_key: true
          tags: [ "audit" ]
          tail_files: true
          parsers:
          - multiline:
              type: pattern
              pattern: "^[0-9]{4}-[0-9]{2}-[0-9]{2}"
              negate: false
              match: after
              max_lines: 1000
              timeout: 5s
        filebeat.config.modules:
          path: ${path.config}/modules.d/*.yml
          reload.enabled: true
          reload.period: 10s
        setup.template.settings:
          index.number_of_shards: 1
        processors:
        - add_host_metadata: {}
        - add_cloud_metadata: {}
        output.logstash:
          # This needs to be {{logstash-name}}-ls-beats:5044
          hosts: [ "eck-stack-eck-logstash-ls-beats.elastic-system.svc:5044" ]
      # deployment: null

  eck-logstash:
    enabled: true
    version: 8.19.9
    config:
      log.level: info
      queue.type: persisted
      path.queue: /usr/share/logstash/dp
      dead_letter_queue.max_bytes: 10gb
      # pipeline.buffer.type: heap

    podTemplate:
      metadata:
        annotations:
          # Disable Stakater Reloader - ECK manages its own config updates
          reloader.stakater.com/auto: "false"
      spec:
        # Override init container resources to fix SMT alignment error
        # Your nodes require CPU requests to be multiples of 2 (SMT cores per physical core)
        initContainers:
        - name: logstash-internal-init-config
          resources:
            requests:
              cpu: 2
              memory: 128Mi
            limits:
              cpu: 2
              memory: 256Mi
        - name: elastic-internal-init-keystore
          resources:
            requests:
              cpu: 2
              memory: 1Gi
            limits:
              cpu: 2
              memory: 2Gi
        containers:
        - name: logstash
          resources:
            # Memory limit should be ~2x heap size to account for off-heap usage
            # JVM heap is 4GB, limit is 8GB for complex filters with JSON parsing
            requests:
              memory: 8Gi
              cpu: 2
            limits:
              memory: 8Gi
              cpu: 2
          env:
          - name: ES_USER
            value: "elastic"
          - name: ES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: elasticsearch-es-elastic-user
                key: elastic
          # JVM options: 4GB heap with maxOrder=14 to handle large Filebeat batches
          - name: LS_JAVA_OPTS
            value: "-Xms4g -Xmx4g -Dio.netty.allocator.maxOrder=14"
          volumeMounts:
          - mountPath: /usr/share/logstash/pq
            name: pq
            readOnly: false
          - mountPath: /usr/share/logstash/dlq
            name: dlq
            readOnly: false
          - mountPath: /etc/logstash/config/certs
            name: certs
            readOnly: true
        volumes:
        - name: certs
          secret:
            secretName: elasticsearch-es-cert
    pipelines:
    - pipeline.id: main
      dead_letter_queue.enable: true
      path.dead_letter_queue: /usr/share/logstash/dlq
      config.reload.automatic: true
      config.reload.interval: 30s
      dead_letter_queue.storage_policy: drop_older
      dead_letter_queue.max_bytes: 5gb
      dead_letter_queue.retain.age: 10m
      pipeline.ecs_compatibility: disabled
      config.string: |
        input {
          beats {
            port => 5044
          }
        }

        filter {
          #===========================================
          # AUDIT LOGS PROCESSING
          #===========================================
          if "audit" in [tags] {
            # Parse the JSON audit log message
            json {
              source => "message"
              target => "audit"
              skip_on_invalid_json => true
            }

            if [audit] {
              # Extract key audit fields to top level for easy querying
              mutate {
                add_field => {
                  "audit_id" => "%{[audit][auditID]}"
                  "audit_verb" => "%{[audit][verb]}"
                  "audit_user" => "%{[audit][user][username]}"
                  "audit_resource" => "%{[audit][objectRef][resource]}"
                  "audit_namespace" => "%{[audit][objectRef][namespace]}"
                  "audit_name" => "%{[audit][objectRef][name]}"
                  "audit_api_version" => "%{[audit][objectRef][apiVersion]}"
                  "audit_request_uri" => "%{[audit][requestURI]}"
                }
              }

              # Extract response code if present
              if [audit][responseStatus][code] {
                mutate {
                  add_field => { "audit_response_code" => "%{[audit][responseStatus][code]}" }
                }
                mutate {
                  convert => { "audit_response_code" => "integer" }
                }
              }

              # Categorize audit action
              if [audit][verb] == "create" {
                mutate { add_field => { "audit_action" => "CREATE" } }
              } else if [audit][verb] == "delete" {
                mutate { add_field => { "audit_action" => "DELETE" } }
              } else if [audit][verb] == "update" or [audit][verb] == "patch" {
                mutate { add_field => { "audit_action" => "UPDATE" } }
              } else if [audit][verb] == "get" or [audit][verb] == "list" or [audit][verb] == "watch" {
                mutate { add_field => { "audit_action" => "READ" } }
              } else {
                mutate { add_field => { "audit_action" => "OTHER" } }
              }

              # Remove verbose/unnecessary nested fields
              mutate {
                remove_field => [
                  "[audit][annotations]",
                  "[audit][apiVersion]",
                  "[audit][kind]",
                  "[audit][level]",
                  "[audit][stage]",
                  "[audit][stageTimestamp]",
                  "[audit][userAgent]",
                  "[audit][user][extra]",
                  "[audit][responseStatus][message]",
                  "message"
                ]
              }
            }

            mutate {
              add_field => { "[@metadata][target_index]" => "audit-logs" }
            }
          }

          #===========================================
          # CONTAINER LOGS PROCESSING
          #===========================================
          else {
            # Extract Kubernetes metadata to top-level fields
            if [kubernetes] {
              mutate {
                add_field => {
                  "namespace" => "%{[kubernetes][namespace]}"
                  "pod" => "%{[kubernetes][pod][name]}"
                  "container" => "%{[kubernetes][container][name]}"
                  "node" => "%{[kubernetes][node][name]}"
                }
              }

              # Extract app name from labels if available
              if [kubernetes][labels][app] {
                mutate { add_field => { "app" => "%{[kubernetes][labels][app]}" } }
              } else if [kubernetes][labels][app_kubernetes_io/name] {
                mutate { add_field => { "app" => "%{[kubernetes][labels][app_kubernetes_io/name]}" } }
              } else {
                mutate { add_field => { "app" => "%{[kubernetes][container][name]}" } }
              }
            }

            # Try to parse JSON logs (many apps output structured JSON)
            if [message] =~ /^\s*\{/ {
              json {
                source => "message"
                target => "json_log"
                skip_on_invalid_json => true
              }

              if [json_log] {
                # Extract common JSON log fields
                if [json_log][level] {
                  mutate { add_field => { "log_level" => "%{[json_log][level]}" } }
                } else if [json_log][severity] {
                  mutate { add_field => { "log_level" => "%{[json_log][severity]}" } }
                } else if [json_log][lvl] {
                  mutate { add_field => { "log_level" => "%{[json_log][lvl]}" } }
                }

                if [json_log][msg] {
                  mutate { add_field => { "log_message" => "%{[json_log][msg]}" } }
                } else if [json_log][message] {
                  mutate { add_field => { "log_message" => "%{[json_log][message]}" } }
                }

                if [json_log][error] {
                  mutate { add_field => { "error_message" => "%{[json_log][error]}" } }
                } else if [json_log][err] {
                  mutate { add_field => { "error_message" => "%{[json_log][err]}" } }
                }

                if [json_log][stack_trace] {
                  mutate { add_field => { "stack_trace" => "%{[json_log][stack_trace]}" } }
                } else if [json_log][stacktrace] {
                  mutate { add_field => { "stack_trace" => "%{[json_log][stacktrace]}" } }
                }
              }
            }

            # Extract log level from message if not already set
            if ![log_level] {
              grok {
                match => { "message" => "(?i)(?<log_level>FATAL|ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE)" }
                tag_on_failure => []
              }
            }

            # Normalize log levels to uppercase
            if [log_level] {
              mutate {
                uppercase => [ "log_level" ]
              }
              # Normalize WARNING to WARN
              if [log_level] == "WARNING" {
                mutate { replace => { "log_level" => "WARN" } }
              }
            } else {
              mutate { add_field => { "log_level" => "INFO" } }
            }

            # Categorize log severity for alerting
            if [log_level] == "FATAL" or [log_level] == "ERROR" {
              mutate { add_field => { "log_severity" => "error" } }
            } else if [log_level] == "WARN" {
              mutate { add_field => { "log_severity" => "warning" } }
            } else {
              mutate { add_field => { "log_severity" => "info" } }
            }

            # Clean up - remove verbose nested fields
            mutate {
              remove_field => [
                "[kubernetes][namespace_labels]",
                "[kubernetes][namespace_uid]",
                "[kubernetes][node][labels]",
                "[kubernetes][node][uid]",
                "[kubernetes][node][hostname]",
                "[kubernetes][pod][ip]",
                "[kubernetes][pod][uid]",
                "[kubernetes][labels]",
                "[kubernetes][replicaset]",
                "[kubernetes][statefulset]",
                "[kubernetes][daemonset]",
                "[kubernetes][deployment]",
                "[container][runtime]",
                "[container][id]",
                "[container][image]",
                "[event]",
                "[service]",
                "[ecs]",
                "[agent]",
                "[input]",
                "[log][offset]",
                "[log][file]",
                "_grokparsefailure"
              ]
            }

            # Remove json_log after extraction to reduce document size
            mutate {
              remove_field => [ "json_log" ]
            }

            mutate {
              add_field => { "[@metadata][target_index]" => "container-logs" }
            }
          }

          # Remove common unnecessary fields from all logs
          mutate {
            remove_field => [ "@version", "host" ]
          }
        }

        output {
          if [@metadata][target_index] == "container-logs" {
            elasticsearch {
              hosts => [ "https://elasticsearch-es-http.elastic-system.svc:9200" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              ilm_rollover_alias => "container-logs"
              ilm_pattern => "000001"
              ilm_enabled => true
              ilm_policy => "hot_warm_delete-container-logs"
              data_stream => auto
              ssl_enabled => true
              ssl_certificate_authorities => ["/etc/logstash/config/certs/ca.crt"]
            }
          } else if [@metadata][target_index] == "audit-logs" {
            elasticsearch {
              hosts => [ "https://elasticsearch-es-http.elastic-system.svc:9200" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              ilm_enabled => true
              ilm_rollover_alias => "audit-logs"
              ilm_pattern => "000001"
              ilm_policy => "hot_warm_delete-audit-logs"
              ssl_enabled => true
              data_stream => auto
              ssl_certificate_authorities => ["/etc/logstash/config/certs/ca.crt"]
            }
          }
        }
    volumeClaimTemplates:
    - metadata:
        name: pq
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 15Gi
    - metadata:
        name: dlq
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi

    elasticsearchRefs:
    - clusterName: eck
      name: elasticsearch
    secureSettings:
    - secretName: elasticsearch-es-cert
    services:
    - name: beats
      service:
        spec:
          type: ClusterIP
          ports:
          - port: 5044
            name: "filebeat"
            protocol: TCP
            targetPort: 5044
