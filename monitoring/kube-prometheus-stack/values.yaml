# ────────────────────────────────────────────────────────────────────────────────
# Global settings for Bitnami charts (Thanos)
# ────────────────────────────────────────────────────────────────────────────────
global:
  security:
    allowInsecureImages: true

# ────────────────────────────────────────────────────────────────────────────────
# kube-prometheus-stack (Prometheus, Alertmanager, Grafana)
# Community chart - no Rancher dependencies
# ────────────────────────────────────────────────────────────────────────────────
kube-prometheus-stack:
  fullnameOverride: "kps"

  # ─────────────────────────────────────────────────────────────────────────────
  # Grafana Configuration
  # ─────────────────────────────────────────────────────────────────────────────
  grafana:
    enabled: true
    fullnameOverride: "grafana"

    env:
      GF_SERVER_ROOT_URL: https://grafana.ops.techsecom.io
      GF_SECURITY_COOKIE_SAMESITE: lax
      TZ: America/Chicago
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: cnpg-cluster-rw.cnpg-system.svc.cluster.local:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_SSL_MODE: disable

    admin:
      existingSecret: grafana-admin-creds
      userKey: username
      passwordKey: password

    plugins:
      - grafana-exploretraces-app

    assertNoLeakedSecrets: false
    initChownData:
      enabled: false

    persistence:
      enabled: true
      type: statefulset
      storageClassName: nfs-csi
      accessModes:
        - ReadWriteOnce
      size: 10Gi

    service:
      type: ClusterIP
      port: 80

    grafana.ini:
      server:
        root_url: https://grafana.ops.techsecom.io
      auth.anonymous:
        enabled: false
      dataproxy:
        max_idle_connections: 500
      security:
        allow_embedding: true
      feature_toggles:
        provisioning: true
        kubernetesDashboards: true
      auth:
        disable_signout_menu: false
        oauth_auto_login: true
        signout_redirect_url: https://grafana.ops.techsecom.io/login/generic_oauth
      auth.generic_oauth:
        enabled: true
        name: OAuth
        client_id: $__env{GF_AUTH_GENERIC_OAUTH_CLIENT_ID}
        client_secret: $__env{GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET}
        allow_sign_up: true
        scopes: openid profile email roles
        auth_url: $__env{GF_AUTH_GENERIC_OAUTH_AUTH_URL}
        token_url: $__env{GF_AUTH_GENERIC_OAUTH_TOKEN_URL}
        api_url: $__env{GF_AUTH_GENERIC_OAUTH_API_URL}
        use_pkce: true
        use_refresh_token: true
        role_attribute_path: contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'
        role_attribute_strict: false
        allow_assign_grafana_admin: true
      users:
        auto_assign_org: true
        auto_assign_org_id: 1
        auto_assign_org_role: Editor
      auth.basic:
        enabled: false
      database:
        type: postgres
        host: "cnpg-cluster-rw.cnpg-system.svc.cluster.local:5432"
        name: grafana
        user: $__file{/etc/secrets/username}
        password: $__file{/etc/secrets/password}
        ssl_mode: disable
      plugins:
        allow_loading_unsigned_plugins: grafana-pyroscope-app,grafana-exploretraces-app

    extraSecretMounts:
      - name: grafana-db-secret
        secretName: grafana-user-secret
        mountPath: /etc/secrets
        readOnly: true

    # Environment variables from secrets
    envFromSecrets:
      - name: grafana-user-secret
      - name: grafana-oauth-secret

    # Additional env vars with valueFrom
    envValueFrom:
      GF_DATABASE_USER:
        secretKeyRef:
          name: grafana-user-secret
          key: username
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
        secretKeyRef:
          name: grafana-oauth-secret
          key: client_id
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
        secretKeyRef:
          name: grafana-oauth-secret
          key: client_secret
      GF_AUTH_GENERIC_OAUTH_AUTH_URL:
        secretKeyRef:
          name: grafana-oauth-secret
          key: auth_url
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL:
        secretKeyRef:
          name: grafana-oauth-secret
          key: token_url
      GF_AUTH_GENERIC_OAUTH_API_URL:
        secretKeyRef:
          name: grafana-oauth-secret
          key: api_url

    # Simple env vars
    extraEnv:
      - name: GF_DATABASE_PASSWORD__FILE
        value: /etc/secrets/password

    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      hosts:
        - grafana.ops.techsecom.io
      tls:
        - hosts:
            - grafana.ops.techsecom.io
          secretName: grafana-cert-tls

    serviceMonitor:
      enabled: true

    serviceAccount:
      autoMount: true
      create: true

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
        provider:
          allowUiUpdates: false
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        label: grafana_datasource
        labelValue: "1"

    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: utc

    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi

    # Additional datasources for Thanos, Loki, Tempo
    additionalDataSources:
      - name: Thanos
        uid: thanos
        type: prometheus
        access: proxy
        url: http://kube-prometheus-stack-thanos-query.monitoring.svc.cluster.local:9090
        isDefault: false
        jsonData:
          httpMethod: POST
          exemplarTraceIdDestinations:
            - datasourceUid: tempo
              name: Tempo
        editable: false

      - name: Loki
        uid: loki
        type: loki
        access: proxy
        url: http://kube-prometheus-stack-loki-gateway.monitoring.svc.cluster.local
        isDefault: false
        jsonData:
          httpHeaderName1: "X-scope-OrgID"
          derivedFields:
            - name: trace_id
              matcherRegex: '"trace_id"\\s*:\\s*"([a-fA-F0-9\\-]+)"'
              datasourceUid: tempo
              internalLink: true
            - name: traceId
              matcherRegex: '"traceId"\\s*:\\s*"([a-fA-F0-9\\-]+)"'
              datasourceUid: tempo
              internalLink: true
        editable: false

      - name: Tempo
        uid: tempo
        type: tempo
        access: proxy
        url: http://tempo.monitoring.svc.cluster.local:3200
        isDefault: false
        jsonData:
          httpMethod: GET
          tracesToLogsV2:
            datasourceUid: loki
          search:
            hide: false
          lokiSearch:
            datasourceUid: loki
          tracesToLogs:
            datasourceUid: loki
            mapTagNamesEnabled: true
            tags: ["namespace", "pod", "container", "audit", "app"]
            filterByTraceID: true
            filterBySpanID: true
          serviceMap:
            datasourceUid: thanos
          nodeGraph:
            enabled: true
        editable: false

  # ─────────────────────────────────────────────────────────────────────────────
  # Prometheus Configuration
  # ─────────────────────────────────────────────────────────────────────────────
  prometheus:
    enabled: true

    thanosService:
      enabled: true

    service:
      type: ClusterIP
      port: 9090

    ingress:
      enabled: true
      pathType: Prefix
      ingressClassName: nginx
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      hosts:
        - prometheus.ops.techsecom.io
      tls:
        - hosts:
            - prometheus.ops.techsecom.io
          secretName: prom-server-cert-tls

    prometheusSpec:
      externalLabels:
        cluster: "ops-dc-tx-rke2-mgmt-prod"

      # Thanos sidecar for long-term storage
      thanos:
        image: quay.io/thanos/thanos:v0.39.2
        version: "v0.39.2"
        objectStorageConfig:
          existingSecret:
            name: thanos-objstore-secret
            key: objstore.yml

      enableFeatures:
        - auto-gomemlimit
        - memory-snapshot-on-shutdown
        - new-service-discovery-manager
        - exemplar-storage
        - native-histograms

      # Pick up all ServiceMonitors/PodMonitors across namespaces
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      scrapeConfigSelectorNilUsesHelmValues: false

      enableRemoteWriteReceiver: true
      enableAdminAPI: true
      replicas: 1

      scrapeInterval: 30s
      evaluationInterval: 30s
      retention: 10d
      retentionSize: 50GiB

      # Probe configuration to prevent restarts during heavy load
      # Default 3s timeout is too aggressive for 800k+ time series workloads
      # With 805k series, chunk snapshots alone take 5+ seconds
      startupProbe:
        failureThreshold: 60
        periodSeconds: 15
        timeoutSeconds: 10
      livenessProbe:
        failureThreshold: 10
        periodSeconds: 10
        timeoutSeconds: 10
      readinessProbe:
        failureThreshold: 5
        periodSeconds: 10
        timeoutSeconds: 10

      resources:
        limits:
          cpu: 4000m
          memory: 8Gi
        requests:
          cpu: 1000m
          memory: 4Gi

      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
            storageClassName: nfs-csi

      securityContext:
        fsGroup: 2000
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault

      additionalScrapeConfigs:
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090
        - job_name: kubernetes-apiservers
          kubernetes_sd_configs:
            - role: endpoints
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

    serviceMonitor:
      selfMonitor: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Alertmanager Configuration
  # ─────────────────────────────────────────────────────────────────────────────
  alertmanager:
    enabled: true

    # Minimal global config - routing/receivers defined via AlertmanagerConfig CR
    # which supports webhookUrl secret reference for MS Teams
    config:
      global:
        resolve_timeout: 5m

    ingress:
      enabled: true
      pathType: Prefix
      ingressClassName: nginx
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      hosts:
        - alertmanager.ops.techsecom.io
      tls:
        - hosts:
            - alertmanager.ops.techsecom.io
          secretName: alertmanager-cert-tls

    alertmanagerSpec:
      replicas: 1
      retention: 120h
      externalUrl: https://alertmanager.ops.techsecom.io

      # Pick up AlertmanagerConfig CRs from the monitoring namespace
      # This allows using webhookUrl secret reference for MS Teams
      alertmanagerConfigSelector:
        matchLabels:
          app: alertmanager
      alertmanagerConfigNamespaceSelector: {}

      # Disable automatic namespace matcher so our config matches all alerts
      alertmanagerConfigMatcherStrategy:
        type: None

      secrets:
        - alertmanager-msteams-secret

      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-csi
            resources:
              requests:
                storage: 10Gi

      resources:
        limits:
          cpu: 1000m
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 100Mi

      securityContext:
        fsGroup: 2000
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault

    serviceMonitor:
      selfMonitor: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Prometheus Operator Configuration
  # ─────────────────────────────────────────────────────────────────────────────
  prometheusOperator:
    enabled: true

    admissionWebhooks:
      enabled: true
      patch:
        enabled: true

    serviceMonitor:
      selfMonitor: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Node Exporter
  # ─────────────────────────────────────────────────────────────────────────────
  nodeExporter:
    enabled: true

  prometheus-node-exporter:
    prometheus:
      monitor:
        enabled: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Kube State Metrics
  # ─────────────────────────────────────────────────────────────────────────────
  kubeStateMetrics:
    enabled: true

  kube-state-metrics:
    prometheus:
      monitor:
        enabled: true
        honorLabels: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Kubernetes Components Monitoring
  # RKE2 requires --bind-address=0.0.0.0 for kube-controller-manager and
  # kube-scheduler in /etc/rancher/rke2/config.yaml to expose metrics
  # ─────────────────────────────────────────────────────────────────────────────
  kubeControllerManager:
    enabled: true
    endpoints:
      - 10.0.3.10
      - 10.0.3.11
      - 10.0.3.12
    service:
      enabled: true
      port: 10257
      targetPort: 10257
    serviceMonitor:
      enabled: true
      https: true
      insecureSkipVerify: true

  kubeScheduler:
    enabled: true
    endpoints:
      - 10.0.3.10
      - 10.0.3.11
      - 10.0.3.12
    service:
      enabled: true
      port: 10259
      targetPort: 10259
    serviceMonitor:
      enabled: true
      https: true
      insecureSkipVerify: true

  # Disabled: Using Cilium kube-proxy replacement
  kubeProxy:
    enabled: false

  kubeEtcd:
    enabled: true
    endpoints:
      - 10.0.3.10
      - 10.0.3.11
      - 10.0.3.12
    service:
      enabled: true
      port: 2381
      targetPort: 2381
    serviceMonitor:
      enabled: true
      scheme: http

  kubeDns:
    enabled: false

  kubeApiServer:
    enabled: true

  kubelet:
    enabled: true
    namespace: kube-system

  coreDns:
    enabled: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Default Rules
  # ─────────────────────────────────────────────────────────────────────────────
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerMemoryWorkingSetBytes: true
      k8sContainerResource: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubeProxy: false  # Disabled: Using Cilium kube-proxy replacement
      kubeSchedulerAlerting: true
      kubeSchedulerRecording: true
      kubeStateMetrics: true
      kubelet: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true

# ────────────────────────────────────────────────────────────────────────────────
# Bitnami Thanos (Query, Store Gateway, Compactor)
# ────────────────────────────────────────────────────────────────────────────────
thanos:
  image:
    registry: quay.io
    repository: thanos/thanos
    tag: v0.39.2

  query:
    enabled: true
    replicaCount: 2
    replicaLabels:
      - replica
    dnsDiscovery:
      sidecarsService: kps-thanos-discovery
      sidecarsNamespace: monitoring
    ingress:
      enabled: true
      hostname: thanos.ops.techsecom.io
      annotations:
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      ingressClassName: nginx
      tls: true

  queryFrontend:
    enabled: true

  bucketweb:
    enabled: true
    ingress:
      enabled: true
      hostname: thanos-bucketweb.ops.techsecom.io
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      ingressClassName: nginx
      tls: true

  compactor:
    enabled: true
    retentionResolutionRaw: 7d
    retentionResolution5m: 14d
    retentionResolution1h: 30d
    # Increased resources to prevent OOMKills during block compaction
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    # Extended probe timeouts to prevent restarts during heavy block operations
    # Block sync can take 1+ minute during compaction
    livenessProbe:
      enabled: true
      initialDelaySeconds: 60
      periodSeconds: 30
      timeoutSeconds: 60
      failureThreshold: 10
      successThreshold: 1
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 30
      timeoutSeconds: 60
      failureThreshold: 10
      successThreshold: 1
    ingress:
      enabled: true
      hostname: thanos-compactor.ops.techsecom.io
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      ingressClassName: nginx
      tls: true
    persistence:
      enabled: true
      storageClass: nfs-csi
      size: 20Gi

  storegateway:
    enabled: true
    ingress:
      enabled: true
      hostname: thanos-storegateway.ops.techsecom.io
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      ingressClassName: nginx
      tls: true
    persistence:
      enabled: true
      storageClass: nfs-csi
      size: 7Gi

  ruler:
    enabled: false

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

  existingObjstoreSecret: thanos-objstore-secret

# ────────────────────────────────────────────────────────────────────────────────
# Loki (Log Aggregation)
# ────────────────────────────────────────────────────────────────────────────────
loki:
  deploymentMode: SimpleScalable

  loki:
    auth_enabled: false

    analytics:
      reporting_enabled: false

    compactor:
      working_directory: /var/loki/compactor/retention
      delete_request_store: s3
      retention_enabled: true

    frontend:
      max_outstanding_per_tenant: 4096

    ingester:
      chunk_encoding: snappy

    commonConfig:
      replication_factor: 2

    limits_config:
      ingestion_burst_size_mb: 128
      ingestion_rate_mb: 64
      max_query_parallelism: 100
      per_stream_rate_limit: 64M
      per_stream_rate_limit_burst: 128M
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      retention_period: 14d
      shard_streams:
        enabled: true
      split_queries_by_interval: 1h
      max_line_size: 5242880
      max_line_size_truncate: true
      max_global_streams_per_user: 50000
      max_streams_per_user: 10000

    query_scheduler:
      max_outstanding_requests_per_tenant: 4096

    rulerConfig:
      enable_api: true
      enable_alertmanager_v2: true
      alertmanager_url: http://kps-alertmanager.monitoring.svc.cluster.local:9093
      storage:
        type: local
        local:
          directory: /rules
      rule_path: /rules/fake

    schemaConfig:
      configs:
        - from: "2024-04-01"
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h

    server:
      log_level: info
      grpc_server_max_recv_msg_size: 8388608
      grpc_server_max_send_msg_size: 8388608

    storage:
      type: s3
      bucketNames:
        chunks: loki-chunks
        ruler: loki-ruler
        admin: loki-admin
      s3:
        s3ForcePathStyle: true
        insecure: true
        endpoint: http://minio.minio.svc.cluster.local:9000
        region: us-east-1
        # Credentials are provided via AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
        # environment variables from loki-s3-secret (mounted via extraEnvFrom)

  gateway:
    replicas: 2
    enabled: true
    image:
      registry: docker.io
      repository: nginxinc/nginx-unprivileged
      tag: 1.29-alpine
    nginxConfig:
      resolver: "rke2-coredns-rke2-coredns.kube-system.svc.cluster.local valid=30s ipv6=off"

  write:
    replicas: 3
    persistence:
      size: 10Gi
      storageClass: "nfs-csi"
    extraEnvFrom:
      - secretRef:
          name: loki-s3-secret
    resources:
      limits:
        cpu: "2"
        memory: 2Gi
      requests:
        cpu: 100m
        memory: 256Mi

  read:
    replicas: 2
    extraEnvFrom:
      - secretRef:
          name: loki-s3-secret
    resources:
      limits:
        cpu: "2"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi

  backend:
    replicas: 2
    persistence:
      storageClass: "nfs-csi"
    extraEnvFrom:
      - secretRef:
          name: loki-s3-secret
    resources:
      limits:
        cpu: "2"
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 256Mi

  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      cert-manager.io/revision-history-limit: "3"
      external-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/duration: "2160h"
      cert-manager.io/renew-before: "720h"
    paths:
      distributor:
        - /api/prom/push
        - /loki/api/v1/push
        - /otlp/v1/logs
      queryFrontend:
        - /api/prom/query
        - /api/prom/label
        - /api/prom/series
        - /api/prom/tail
        - /loki/api/v1/query
        - /loki/api/v1/query_range
        - /loki/api/v1/tail
        - /loki/api/v1/label
        - /loki/api/v1/labels
        - /loki/api/v1/series
        - /loki/api/v1/index/stats
    hosts:
      - loki.ops.techsecom.io
    tls:
      - secretName: loki-distributed-tls
        hosts:
          - loki.ops.techsecom.io

  monitoring:
    dashboards:
      enabled: true
      labels:
        grafana_dashboard: "1"
    rules:
      enabled: true
      alerting: true
    serviceMonitor:
      enabled: true
    podMonitor:
      enabled: true

  lokiCanary:
    enabled: false

  test:
    enabled: false

  # Memcached chunk cache - memory reduced to fit within LimitRange (max 8Gi)
  chunksCache:
    enabled: true
    replicas: 1
    allocatedMemory: 7680  # 7.5Gi in MB to allow headroom
    resources:
      requests:
        cpu: 500m
        memory: 7680Mi
      limits:
        cpu: "2"
        memory: 8Gi

  # Results cache - using default values (fits within limit)
  resultsCache:
    enabled: true
    replicas: 1

  sidecar:
    image:
      repository: ghcr.io/kiwigrid/k8s-sidecar
    rules:
      searchNamespace: ALL
      folder: /rules/fake

# ────────────────────────────────────────────────────────────────────────────────
# Tempo (Distributed Tracing)
# ────────────────────────────────────────────────────────────────────────────────
tempo:
  enabled: true
  fullnameOverride: tempo

  serviceMonitor:
    enabled: true

  stream_over_http_enabled: true

  gateway:
    enabled: true

  minio:
    enabled: false

  storage:
    trace:
      backend: s3
      s3:
        bucket: "tempo-traces"
        endpoint: "http://minio.minio.svc.cluster.local:9000"
        insecure: true
        # Credentials are provided via AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
        # environment variables from tempo-storage-secret (mounted via extraEnvFrom)

  extraEnvFrom:
    - secretRef:
        name: tempo-storage-secret

  traces:
    otlp:
      http:
        enabled: true
      grpc:
        enabled: true

  distributor:
    config:
      log_received_spans:
        enabled: true
      log_discarded_spans:
        enabled: true

  tempo:
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://kps-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"

  persistence:
    enabled: true
    size: 20Gi
    storageClassName: nfs-csi

# ────────────────────────────────────────────────────────────────────────────────
# Promtail (Log Collection)
# ────────────────────────────────────────────────────────────────────────────────
promtail:
  enabled: true

  tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists

  # SELinux context to allow reading host log files
  # spc_t (super privileged container) allows access to host paths
  containerSecurityContext:
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    seLinuxOptions:
      type: spc_t

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  config:
    logLevel: info
    serverPort: 3101
    clients:
      - url: http://kube-prometheus-stack-loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push
        backoff_config:
          min_period: 500ms
          max_period: 5m
          max_retries: 10
        batchwait: 1s
        batchsize: 1048576
        timeout: 10s
        tenant_id: ""

    snippets:
      extraRelabelConfigs:
        - action: replace
          replacement: ops-dc-tx-rke2-mgmt-prod
          target_label: cluster

      pipelineStages:
        - cri: {}
        - json:
            expressions:
              level: level
              msg: msg
              message: message
        - labels:
            level:
        - regex:
            expression: '(?i)(?P<extracted_level>FATAL|ERROR|WARN(?:ING)?|INFO|DEBUG|TRACE)'
        - labels:
            extracted_level:

      extraScrapeConfigs: |
        - job_name: rancher-audit
          static_configs:
            - targets:
                - localhost
              labels:
                job: rancher-audit
                log_type: audit
                __path__: /var/log/rancher/*.log
          pipeline_stages:
            - json:
                expressions:
                  stage: stage
                  verb: verb
                  resource: objectRef.resource
            - drop:
                source: stage
                expression: 'RequestReceived'
            - labels:
                verb:
                resource:
        - job_name: linux-audit
          static_configs:
            - targets:
                - localhost
              labels:
                job: linux-audit
                log_type: audit
                __path__: /var/log/audit/audit.log
          pipeline_stages:
            - regex:
                expression: 'type=(?P<audit_type>\w+).*msg=audit\((?P<timestamp>[^:]+):(?P<audit_id>\d+)\).*'
            - labels:
                audit_type:
        - job_name: systemd-journal
          journal:
            max_age: 12h
            labels:
              job: systemd-journal
              log_type: journal
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              target_label: 'unit'
            - source_labels: ['__journal__hostname']
              target_label: 'hostname'
            - source_labels: ['__journal_priority_keyword']
              target_label: 'level'

  defaultVolumes:
    - name: run
      emptyDir: {}  # Use emptyDir instead of hostPath to fix permission issues
    - name: containers
      hostPath:
        path: /var/lib/docker/containers
    - name: pods
      hostPath:
        path: /var/log/pods
    - name: journal
      hostPath:
        path: /var/log/journal
    - name: machine-id
      hostPath:
        path: /etc/machine-id
    - name: audit-logs
      hostPath:
        path: /var/log/audit
    - name: rancher-audit-logs
      hostPath:
        path: /var/log/rancher

  defaultVolumeMounts:
    - name: run
      mountPath: /run/promtail
    - name: containers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: pods
      mountPath: /var/log/pods
      readOnly: true
    - name: journal
      mountPath: /var/log/journal
      readOnly: true
    - name: machine-id
      mountPath: /etc/machine-id
      readOnly: true
    - name: audit-logs
      mountPath: /var/log/audit
      readOnly: true
    - name: rancher-audit-logs
      mountPath: /var/log/rancher
      readOnly: true

  serviceMonitor:
    enabled: true

  extraPorts:
    syslog:
      name: tcp-syslog
      containerPort: 1514
      protocol: TCP
      service:
        type: ClusterIP
        port: 1514

# ────────────────────────────────────────────────────────────────────────────────
# Alloy (OpenTelemetry Collector)
# ────────────────────────────────────────────────────────────────────────────────
alloy:
  alloy:
    mounts:
      varlog: true

    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "12345"
        prometheus.io/path: "/metrics"

    extraPorts:
      - name: otlp-grpc
        port: 4317
        targetPort: 4317
        protocol: TCP
      - name: otlp-http
        port: 4318
        targetPort: 4318
        protocol: TCP

    configMap:
      content: |
        // OTLP receiver configuration
        otelcol.receiver.otlp "default" {
          grpc {
            endpoint = "0.0.0.0:4317"
          }
          http {
            endpoint = "0.0.0.0:4318"
          }
          output {
            metrics = [otelcol.exporter.prometheus.default.input]
            traces  = [otelcol.exporter.otlp.tempo.input]
            logs    = [otelcol.exporter.loki.default.input]
          }
        }

        // Metrics to Prometheus
        otelcol.exporter.prometheus "default" {
          forward_to = [prometheus.remote_write.default.receiver]
        }

        prometheus.remote_write "default" {
          endpoint {
            url = "http://kps-prometheus.monitoring.svc.cluster.local/api/v1/write"
          }
        }

        // Traces to Tempo
        otelcol.exporter.otlp "tempo" {
          client {
            endpoint = "http://tempo.monitoring.svc.cluster.local:4317"
            tls {
              insecure = true
            }
          }
        }

        // Logs to Loki
        otelcol.exporter.loki "default" {
          forward_to = [loki.write.endpoint.receiver]
        }

        loki.write "endpoint" {
          endpoint {
            url = "http://kube-prometheus-stack-loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
            batch_size = "1MB"
            batch_wait = "1s"
          }
        }
