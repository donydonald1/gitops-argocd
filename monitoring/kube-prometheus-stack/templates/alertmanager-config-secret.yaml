apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  labels:
    app: alertmanager
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'job', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: "msteams"
      routes:
        - receiver: "null"
          matchers:
            - alertname =~ "Watchdog|InfoInhibitor"
        - receiver: "msteams-critical"
          matchers:
            - severity = "critical"
          group_wait: 10s
          group_interval: 1m
          repeat_interval: 1h
          continue: false
        - receiver: "msteams"
          matchers:
            - severity = "warning"
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 4h
        - receiver: "msteams"
    inhibit_rules:
      - source_matchers:
          - severity = "critical"
        target_matchers:
          - severity = "warning"
        equal: ["alertname", "namespace"]
      - target_match_re:
          alertname: '.+Overcommit'
        source_match:
          alertname: 'Watchdog'
        equal: ['prometheus']
    receivers:
      - name: "null"
      - name: "msteams"
        msteams_configs:
          - send_resolved: true
            webhook_url_file: /etc/alertmanager/secrets/alertmanager-msteams-secret/webhook-url
            title: '{{ "{{" }} if eq .Status "firing" {{ "}}" }}ðŸ”´{{ "{{" }} else {{ "}}" }}ðŸŸ¢{{ "{{" }} end {{ "}}" }} [{{ "{{" }} .Status | toUpper {{ "}}" }}{{ "{{" }} if eq .Status "firing" {{ "}}" }}:{{ "{{" }} .Alerts.Firing | len {{ "}}" }}{{ "{{" }} end {{ "}}" }}] {{ "{{" }} .CommonLabels.alertname {{ "}}" }}'
            text: |
              {{ "{{" }} if eq .Status "firing" {{ "}}" }}
              âš ï¸ **ALERT FIRING** - Immediate attention required
              {{ "{{" }} else {{ "}}" }}
              âœ… **ALERT RESOLVED** - Issue has been remediated
              {{ "{{" }} end {{ "}}" }}

              ---

              **ðŸ·ï¸ Alert Information**

              | Field | Value |
              |-------|-------|
              | **Cluster** | `{{ "{{" }} .CommonLabels.cluster {{ "}}" }}` |
              | **Alert** | `{{ "{{" }} .CommonLabels.alertname {{ "}}" }}` |
              | **Severity** | {{ "{{" }} if eq .CommonLabels.severity "critical" {{ "}}" }}ðŸ”´ **CRITICAL**{{ "{{" }} else if eq .CommonLabels.severity "warning" {{ "}}" }}ðŸŸ¡ **WARNING**{{ "{{" }} else {{ "}}" }}ðŸ”µ **{{ "{{" }} .CommonLabels.severity | toUpper {{ "}}" }}**{{ "{{" }} end {{ "}}" }} |
              {{ "{{" }} if .CommonLabels.namespace {{ "}}" }}| **Namespace** | `{{ "{{" }} .CommonLabels.namespace {{ "}}" }}` |{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if .CommonLabels.service {{ "}}" }}| **Service** | `{{ "{{" }} .CommonLabels.service {{ "}}" }}` |{{ "{{" }} end {{ "}}" }}

              ---

              {{ "{{" }} range $alert := .Alerts {{ "}}" }}

              ---

              **ðŸ“‹ Alert Details**

              {{ "{{" }} if $alert.Annotations.summary {{ "}}" }}
              > **Summary**: {{ "{{" }} $alert.Annotations.summary {{ "}}" }}
              {{ "{{" }} end {{ "}}" }}

              {{ "{{" }} if $alert.Annotations.description {{ "}}" }}
              > **Description**: {{ "{{" }} $alert.Annotations.description {{ "}}" }}
              {{ "{{" }} end {{ "}}" }}

              {{ "{{" }} if $alert.Annotations.runbook_url {{ "}}" }}
              ðŸ“– **Runbook**: [View Runbook]({{ "{{" }} $alert.Annotations.runbook_url {{ "}}" }})
              {{ "{{" }} end {{ "}}" }}

              **Labels:**
              {{ "{{" }} if $alert.Labels.pod {{ "}}" }}- **Pod**: `{{ "{{" }} $alert.Labels.pod {{ "}}" }}`{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if $alert.Labels.container {{ "}}" }}- **Container**: `{{ "{{" }} $alert.Labels.container {{ "}}" }}`{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if $alert.Labels.instance {{ "}}" }}- **Instance**: `{{ "{{" }} $alert.Labels.instance {{ "}}" }}`{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if $alert.Labels.node {{ "}}" }}- **Node**: `{{ "{{" }} $alert.Labels.node {{ "}}" }}`{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if $alert.Labels.job {{ "}}" }}- **Job**: `{{ "{{" }} $alert.Labels.job {{ "}}" }}`{{ "{{" }} end {{ "}}" }}

              **Timestamps:**
              - **Started**: {{ "{{" }} $alert.StartsAt.Format "2006-01-02 15:04:05 MST" {{ "}}" }}
              {{ "{{" }} if $alert.EndsAt.IsZero | not {{ "}}" }}- **Ended**: {{ "{{" }} $alert.EndsAt.Format "2006-01-02 15:04:05 MST" {{ "}}" }}{{ "{{" }} end {{ "}}" }}

              {{ "{{" }} end {{ "}}" }}

              ---

              **ðŸ”— Quick Links**

              - [ðŸ“Š Grafana Dashboard](https://grafana.ops.techsecom.io)
              - [ðŸ”” Alertmanager](https://alertmanager.ops.techsecom.io)
              - [ðŸ“ˆ Prometheus](https://prometheus.ops.techsecom.io)
              - [ðŸ“ Loki Logs](https://grafana.ops.techsecom.io/explore?orgId=1&left=%5B%22now-1h%22,%22now%22,%22Loki%22,%7B%7D%5D)

              ---

              **ðŸ“ž Escalation Path:**
              1. Check Grafana dashboards for anomalies
              2. Review logs in Loki for error context
              3. If critical, escalate to on-call team
              4. Document incident in tracking system
      - name: "msteams-critical"
        msteams_configs:
          - send_resolved: true
            webhook_url_file: /etc/alertmanager/secrets/alertmanager-msteams-secret/webhook-url
            title: 'ðŸš¨ CRITICAL: {{ "{{" }} .CommonLabels.alertname {{ "}}" }} on {{ "{{" }} .CommonLabels.cluster {{ "}}" }}'
            text: |
              **ðŸš¨ðŸš¨ðŸš¨ CRITICAL ALERT - IMMEDIATE ACTION REQUIRED ðŸš¨ðŸš¨ðŸš¨**

              ---

              | Field | Value |
              |-------|-------|
              | **Cluster** | `{{ "{{" }} .CommonLabels.cluster {{ "}}" }}` |
              | **Alert** | `{{ "{{" }} .CommonLabels.alertname {{ "}}" }}` |
              | **Severity** | ðŸ”´ **CRITICAL** |
              {{ "{{" }} if .CommonLabels.namespace {{ "}}" }}| **Namespace** | `{{ "{{" }} .CommonLabels.namespace {{ "}}" }}` |{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if .CommonLabels.pod {{ "}}" }}| **Pod** | `{{ "{{" }} .CommonLabels.pod {{ "}}" }}` |{{ "{{" }} end {{ "}}" }}
              {{ "{{" }} if .CommonLabels.node {{ "}}" }}| **Node** | `{{ "{{" }} .CommonLabels.node {{ "}}" }}` |{{ "{{" }} end {{ "}}" }}

              ---

              {{ "{{" }} range $index, $alert := .Alerts {{ "}}" }}
              {{ "{{" }} if $alert.Annotations.summary {{ "}}" }}> {{ "{{" }} $alert.Annotations.summary {{ "}}" }}{{ "{{" }} end {{ "}}" }}

              {{ "{{" }} if $alert.Annotations.description {{ "}}" }}**Details**: {{ "{{" }} $alert.Annotations.description {{ "}}" }}{{ "{{" }} end {{ "}}" }}

              **Started**: {{ "{{" }} $alert.StartsAt.Format "2006-01-02 15:04:05 MST" {{ "}}" }}
              {{ "{{" }} end {{ "}}" }}

              ---

              **âš¡ Immediate Actions:**
              1. **STOP** - Acknowledge this alert
              2. **ASSESS** - Check [Grafana](https://grafana.ops.techsecom.io) for impact
              3. **ACT** - Follow runbook or escalate immediately
              4. **COMMUNICATE** - Update stakeholders on status

              ðŸ”— [View Alert Source]({{ "{{" }} (index .Alerts 0).GeneratorURL {{ "}}" }})
